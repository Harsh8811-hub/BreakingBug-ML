# -*- coding: utf-8 -*-
"""Untitled34.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bz5ijt6AH9x59ig4BDp5Jaegty0A0V0b
"""

# 1. to handle the data
import pandas as pd
import numpy as np

# 2. To Viusalize the data
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from yellowbrick.cluster import KElbowVisualizer
from matplotlib.colors import ListedColormap

# 3. To preprocess the data
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer

# 4. import Iterative imputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# 5. Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score

# 6. For Classification tasks
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.naive_bayes import GaussianNB

# 7. Metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

import warnings
warnings.filterwarnings('ignore')


df = pd.read_csv("/content/drive/MyDrive/BreakingBug-ML/dataset.csv")

df.head()

# Exploring the data type of each column
df.info()

# Checking the data shape
df.shape

df = df.fillna(method='ffill').fillna(method='bfill')  # Forward fill and backward fill

df['sex'] = df['sex'].str.capitalize()

# Id column stats
df['id'].min(), df['id'].max()

# Age column stats
df['age'].min(), df['age'].max()

df['age'].describe()

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

# Plot the histogram of the age column with custom colors
sns.histplot(df['age'], kde=True, color="#FF5733")
plt.show()

# Plot the mean, median, and mode of the age column
sns.histplot(df['age'], kde=True)
plt.axvline(df['age'].mean(), color='Red', linestyle='--', label='Mean')
plt.axvline(df['age'].median(), color='Green', linestyle='-', label='Median')
plt.axvline(df['age'].mode()[0], color='Blue', linestyle=':', label='Mode')
plt.legend()
plt.show()

# Print the values of mean, median, and mode of the age column
print('Mean:', df['age'].mean())
print('Median:', df['age'].median())
print('Mode:', df['age'].mode()[0])

# Plot the histogram of the age column colored by sex using Plotly
fig = px.histogram(data_frame=df, x='age', color='sex')
fig.show()

# Find the values of the sex column
print(df['sex'].value_counts())

# Calculating the percentage of male and female values in the data
male_count = df['sex'].value_counts().get('Male', 0)
female_count = df['sex'].value_counts().get('Female', 0)

total_count = male_count + female_count

# Calculate percentages
male_percentage = (male_count / total_count) * 100
female_percentage = (female_count / total_count) * 100

# Display the results
print(f'Male percentage in the data: {male_percentage:.2f}%')
print(f'Female percentage in the data: {female_percentage:.2f}%')

# Difference
difference_percentage = ((male_count - female_count) / female_count) * 100
print(f'Males are {difference_percentage:.2f}% more than females in the data.')



# Find the values count of age column grouping by sex column
age_counts_by_sex = df.groupby('sex')['age'].value_counts()
print(age_counts_by_sex)

# Find the unique values in the dataset column
dataset_counts = df['dataset'].value_counts()
print(dataset_counts)

# Plot the countplot of dataset column
fig = px.bar(df, x='dataset', color='sex')
fig.show()

# Print the values of dataset column grouped by sex
print(df.groupby('sex')['dataset'].value_counts())

# Make a plot of age column using Plotly and coloring by dataset
fig = px.histogram(data_frame=df, x='age', color='dataset')
fig.show()

# Print the mean, median, and mode of age column grouped by dataset column
print("___________________________________________________________")
print("Mean of the dataset: ", df.groupby('dataset')['age'].mean())
print("___________________________________________________________")
print("Median of the dataset: ", df.groupby('dataset')['age'].median())
print("___________________________________________________________")
print("Mode of the dataset: ", df.groupby('dataset')['age'].apply(lambda x: x.mode()[0]))
print("___________________________________________________________")

# Value count of cp column
cp_counts = df['cp'].value_counts()
print(cp_counts)

# Count plot of cp column by sex column
sns.countplot(data=df, x='cp', hue='sex')
plt.show()

# Count plot of cp column by dataset column
sns.countplot(data=df, x='cp', hue='dataset')
plt.show()

# Draw the plot of age column group by cp column
fig = px.histogram(data_frame=df, x='age', color='cp')
fig.show()

print(df['trestbps'].describe())

print(f"Percentage of missing values in trestbps column: {df['trestbps'].isnull().sum() / len(df) * 100:.2f}%")

imputer1 = IterativeImputer(max_iter=10, random_state=42)

df['trestbps'] = imputer1.fit_transform(df[['trestbps']])

print(f"Missing values in trestbps column: {df['trestbps'].isnull().sum()}")

df.info()

print((df.isnull().sum() / len(df) * 100).sort_values(ascending=False))

# Create an object of iterative imputer
imputer2 = IterativeImputer(max_iter=10, random_state=42)

# Fit transform on ca, oldpeak, thal, chol, and thalch columns
df['ca'] = imputer2.fit_transform(df[['ca']])
df['oldpeak'] = imputer2.fit_transform(df[['oldpeak']])
df['chol'] = imputer2.fit_transform(df[['chol']])
df['thalch'] = imputer2.fit_transform(df[['thalch']])

# Let's check again for missing values
print((df.isnull().sum() / len(df) * 100).sort_values(ascending=False))

print(f"The missing values in thal column are: {df['thal'].isnull().sum()}")

df['thal'].value_counts()

df.tail()

# Find missing values
print(df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=True))

missing_data_cols = df.isnull().sum()[df.isnull().sum() > 0].index.tolist()
print(missing_data_cols)

# Find categorical columns
cat_cols = df.select_dtypes(include='object').columns.tolist()
print(cat_cols)

# Find numerical columns
num_cols = df.select_dtypes(exclude='object').columns.tolist()
print(num_cols)

print(f'Categorical Columns: {cat_cols}')
print(f'Numerical Columns: {num_cols}')

import pandas as pd
import numpy as np
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, r2_score

# Find columns
categorical_cols = ['sex', 'dataset', 'cp', 'restecg', 'slope', 'ca', 'thal']
bool_cols = ['fbs', 'exang']
numerical_cols = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'num']


# This function imputes missing values in categorical columns
def impute_categorical_missing_data(passed_col):

    df_null = df[df[passed_col].isnull()]
    df_not_null = df[df[passed_col].notnull()]

    X = df_not_null.drop(passed_col, axis=1)
    y = df_not_null[passed_col]

    label_encoder = LabelEncoder()
    if y.dtype == 'object':
        y = label_encoder.fit_transform(y.astype(str))

    imputer = IterativeImputer(estimator=RandomForestClassifier(random_state=16), add_indicator=True)
    X_imputed = imputer.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)

    rf_classifier = RandomForestClassifier()
    rf_classifier.fit(X_train, y_train)
    y_pred = rf_classifier.predict(X_test)
    acc_score = accuracy_score(y_test, y_pred)

    print(f"The feature '{passed_col}' has been imputed with", round(acc_score * 100, 2), "accuracy\n")

    X_null = df_null.drop(passed_col, axis=1)
    X_null_imputed = imputer.transform(X_null)

    if len(df_null) > 0:
        df.loc[df[passed_col].isnull(), passed_col] = rf_classifier.predict(X_null_imputed)

    return df[passed_col]

# This function imputes missing values in continuous columns
def impute_continuous_missing_data(passed_col):

    df_null = df[df[passed_col].isnull()]
    df_not_null = df[df[passed_col].notnull()]

    X = df_not_null.drop(passed_col, axis=1)
    y = df_not_null[passed_col]

    label_encoder = LabelEncoder()
    for col in X.columns:
        if X[col].dtype == 'object':
            X[col] = label_encoder.fit_transform(X[col].astype(str))

    imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=16), add_indicator=True)
    X_imputed = imputer.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)

    rf_regressor = RandomForestRegressor()
    rf_regressor.fit(X_train, y_train)
    y_pred = rf_regressor.predict(X_test)

    print("MAE =", mean_absolute_error(y_test, y_pred))
    print("RMSE =", mean_squared_error(y_test, y_pred, squared=False))
    print("R2 =", r2_score(y_test, y_pred))

    X_null = df_null.drop(passed_col, axis=1)
    for col in X_null.columns:
        if X_null[col].dtype == 'object':
            X_null[col] = label_encoder.fit_transform(X_null[col].astype(str))
    X_null_imputed = imputer.transform(X_null)

    if len(df_null) > 0:
        df.loc[df[passed_col].isnull(), passed_col] = rf_regressor.predict(X_null_imputed)

    return df[passed_col]

# Display missing values sorted
print(df.isnull().sum().sort_values(ascending=False))

import warnings
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score

import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import pandas as pd

warnings.filterwarnings('ignore')

def impute_continuous_missing_data(series):
    """Impute continuous missing data using the mean of the series."""
    return series.fillna(series.mean())

def impute_categorical_missing_data(series):
    """Impute categorical missing data using the mode of the series."""
    return series.fillna(series.mode()[0])

missing_data_cols = ['chol', 'fbs', 'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal']

# Define numeric_cols and categorical_cols
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

# Impute missing values using the functions
for col in missing_data_cols:
    print(f"Missing Values in {col}: {round((df[col].isnull().sum() / len(df)) * 100, 2)}%")

    if col in categorical_cols:
        df[col] = impute_categorical_missing_data(df[col])
    elif col in numeric_cols:
        df[col] = impute_continuous_missing_data(df[col])

print(df.isnull().sum().sort_values(ascending=False))

print("_________________________________________________________________________________________________________________________________________________")

sns.set(rc={"axes.facecolor":"#87CEEB","figure.facecolor":"#EEE8AA"})
palette = ["#682F2F", "#9E726F", "#D6B2B1", "#B9C0C9", "#9F8A78", "#F3AB60"]
cmap = sns.color_palette(palette)

plt.figure(figsize=(15, 12))

# Plotting numerical columns as boxen plots and categorical columns as count plots
for i, col in enumerate(df.columns):
    plt.subplot((len(df.columns) // 3) + 1, 3, i + 1)

    if col in numeric_cols:
        sns.boxenplot(y=df[col], color=palette[i % len(palette)])
        plt.title(col)
    elif col in categorical_cols:
        sns.countplot(y=df[col], palette=cmap)
        plt.title(col)

plt.tight_layout()
plt.show()

print(df[df['trestbps'] == 0])

df = df[df['trestbps'] != 0]

sns.set(rc={"axes.facecolor":"#B76E79","figure.facecolor":"#C0C0C0"})
modified_palette = ["#C44D53", "#B76E79", "#DDA4A5", "#B3BCC4", "#A2867E", "#F3AB60"]
cmap = sns.color_palette(modified_palette)

plt.figure(figsize=(15, 12))

for i, col in enumerate(df.columns):
    plt.subplot((len(df.columns) // 3) + 1, 3, i + 1)

    if col in numeric_cols:
        sns.boxenplot(y=df[col], color=modified_palette[i % len(modified_palette)])
        plt.title(col)
    elif col in categorical_cols:
        sns.countplot(y=df[col], palette=cmap)
        plt.title(col)

plt.tight_layout()
plt.show()

print(df['trestbps'].describe())
print(df.describe())

print("_________________________________________________________________________________________________________________________________________________")

sns.set(rc={"axes.facecolor": "#FFF9ED", "figure.facecolor": "#FFF9ED"})

night_vision_palette = ["#00FF00", "#FF00FF", "#00FFFF", "#FFFF00", "#FF0000", "#0000FF"]

plt.figure(figsize=(15, 12))

for i, col in enumerate(df.columns):
    plt.subplot((len(df.columns) // 3) + 1, 3, i + 1)

    if col in numeric_cols:
        sns.boxenplot(y=df[col], color=night_vision_palette[i % len(night_vision_palette)])
        plt.title(col)
    elif col in categorical_cols:
        sns.countplot(y=df[col], palette=night_vision_palette)
        plt.title(col)

plt.tight_layout()
plt.show()

print(df['age'].describe())

gray_palette = ["#999999", "#666666", "#333333"]

sns.histplot(data=df, x='trestbps', kde=True, color=gray_palette[0])

plt.title('Resting Blood Pressure')
plt.xlabel('Pressure (mmHg)')
plt.ylabel('Count')

plt.style.use('default')
plt.rcParams['figure.facecolor'] = gray_palette[1]
plt.rcParams['axes.facecolor'] = gray_palette[2]

sns.histplot(df, x='trestbps', kde=True, hue='sex', palette="Spectral")

df.info()

print(df.columns)
print(df.head())

# Split the data into X and y
X = df.drop('num', axis=1)
y = df['num']

# Encode categorical columns in X
label_encoders = {}
for col in X.columns:
    if X[col].dtype == 'object':
        label_encoders[col] = LabelEncoder()
        X[col] = label_encoders[col].fit_transform(X[col].astype(str))

# Split the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Import all models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.naive_bayes import GaussianNB

# Importing pipeline
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Import metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold

import warnings
warnings.filterwarnings('ignore')

# Specify column categories
categorical_cols = ['sex', 'dataset', 'cp', 'restecg', 'slope', 'ca', 'thal']
bool_cols = ['fbs', 'exang']
numerical_cols = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak']

# Preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
        ('bool', 'passthrough', bool_cols)
    ])

models = [
    ('Logistic Regression', LogisticRegression(random_state=42, C=0.01, max_iter=200)),
    ('Gradient Boosting', GradientBoostingClassifier(random_state=42, n_estimators=100, learning_rate=0.1)),
    ('KNeighbors Classifier', KNeighborsClassifier(n_neighbors=10)),
    ('Decision Tree Classifier', DecisionTreeClassifier(random_state=42, max_depth=5)),
    ('AdaBoost Classifier', AdaBoostClassifier(random_state=42, n_estimators=50)),
    ('Random Forest', RandomForestClassifier(random_state=42, n_estimators=200, max_depth=7)),
    ('XGBoost Classifier', XGBClassifier(random_state=42, n_estimators=100, max_depth=5)),
    ('Support Vector Machine', SVC(random_state=42, C=1.0, kernel='rbf')),
    ('Naive Bayes Classifier', GaussianNB())
]

best_model = None
best_accuracy = 0.0

# Iterate over the models and evaluate their performance
for name, model in models:
    # Create a pipeline for each model
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    # Perform cross-validation
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(pipeline, X, y, cv=cv)

    # Calculate mean accuracy
    mean_accuracy = scores.mean()

    # Fit the pipeline on the training data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    pipeline.fit(X_train, y_train)

    # Make predictions on the test data
    y_pred = pipeline.predict(X_test)

    # Calculate accuracy score
    accuracy = accuracy_score(y_test, y_pred)

    # Print the performance metrics
    print("Model:", name)
    print("Cross-Validation Accuracy: ", mean_accuracy)
    print("Test Accuracy: ", accuracy)
    print()

    # Check if the current model has the best accuracy
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = pipeline

# Retrieve the best model
print("Best Model: ", best_model)

def evaluate_classification_models(X, y, categorical_columns, bool_columns, numerical_columns):
    # Preprocessing pipeline
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline([
                ('imputer', SimpleImputer(strategy='mean')),
                ('scaler', StandardScaler())
            ]), numerical_columns),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),
            ('bool', 'passthrough', bool_columns)
        ])

    # Split data into train and test sets
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Define models
    models = {
        "Logistic Regression": LogisticRegression(),
        "KNN": KNeighborsClassifier(),
        "NB": GaussianNB(),
        "SVM": SVC(),
        "Decision Tree": DecisionTreeClassifier(),
        "Random Forest": RandomForestClassifier(),
        "XGBoost": XGBClassifier(),
        "Gradient Boosting": GradientBoostingClassifier(),
        "AdaBoost": AdaBoostClassifier()
    }

    # Train and evaluate models
    results = {}
    best_model = None
    best_accuracy = 0.0
    for name, model in models.items():
        pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('model', model)
        ])
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_val)
        accuracy = accuracy_score(y_val, y_pred)
        results[name] = accuracy
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model = name

    return results, best_model

# Example usage:
results, best_model = evaluate_classification_models(X, y, categorical_cols, bool_cols, numerical_cols)
print("Model Accuracies:", results)
print("Best Model:", best_model)

def hyperparameter_tuning(X, y, categorical_columns, models):
    # Define dictionary to store results
    results = {}

    # Specify numeric and categorical columns
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = categorical_columns


    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline([
                ('imputer', SimpleImputer(strategy='mean')),
                ('scaler', StandardScaler())
            ]), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
        ])


    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


    for model_name, model in models.items():

        param_grid = {}
        if model_name == 'Logistic Regression':
            param_grid = {'model__C': [0.1, 1, 10, 100]}
        elif model_name == 'KNN':
            param_grid = {'model__n_neighbors': [3, 5, 7, 9]}
        elif model_name == 'Naive Bayes':
            param_grid = {'model__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]}
        elif model_name == 'SVM':
            param_grid = {'model__C': [0.1, 1, 10, 100], 'model__gamma': [0.1, 1, 10, 100]}
        elif model_name == 'Decision Tree':
            param_grid = {'model__max_depth': [None, 10, 20, 30], 'model__min_samples_split': [2, 5, 10]}
        elif model_name == 'Random Forest':
            param_grid = {'model__n_estimators': [100, 200, 300], 'model__max_depth': [None, 10, 20, 30], 'model__min_samples_split': [2, 5, 10]}
        elif model_name == 'XGBoost':
            param_grid = {'model__learning_rate': [0.01, 0.1, 0.2], 'model__n_estimators': [100, 200, 300], 'model__max_depth': [3, 5, 7]}
        elif model_name == 'Gradient Boosting':
            param_grid = {'model__learning_rate': [0.01, 0.1, 0.2], 'model__n_estimators': [100, 200, 300], 'model__max_depth': [3, 5, 7]}
        elif model_name == 'AdaBoost':
            param_grid = {'model__learning_rate': [0.01, 0.1, 0.2], 'model__n_estimators': [50, 100, 200]}

        # Create pipeline
        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])

        # Perform hyperparameter tuning using GridSearchCV
        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')
        grid_search.fit(X_train, y_train)

        # Get best hyperparameters and evaluate on test set
        best_params = grid_search.best_params_
        best_model = grid_search.best_estimator_
        y_pred = best_model.predict(X_val)
        accuracy = accuracy_score(y_val, y_pred)

        # Store results in dictionary
        results[model_name] = {'best_params': best_params, 'accuracy': accuracy}

    return results

# Define models dictionary
models = {
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "SVM": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "AdaBoost": AdaBoostClassifier()
}


X = df[categorical_cols]
y = df['num']

results = hyperparameter_tuning(X, y, categorical_cols, models)
for model_name, result in results.items():
    print("Model:", model_name)
    print("Best hyperparameters:", result['best_params'])
    print("Accuracy:", result['accuracy'])
    print()